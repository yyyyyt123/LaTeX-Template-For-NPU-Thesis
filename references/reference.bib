@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{sharir2020cost,
  title={The cost of training nlp models: A concise overview},
  author={Sharir, Or and Peleg, Barak and Shoham, Yoav},
  journal={arXiv preprint arXiv:2004.08900},
  year={2020}
}

@misc{deepspeed,
    title={DeepSpeed},
    url={https://www.deepspeed.ai/}
}

@misc{nccl,
    title={NCCL},
    url={https://github.com/NVIDIA/nccl}
}

@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International Conference on Machine Learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}
@article{he2021fastmoe,
  title={Fastmoe: A fast mixture-of-expert training system},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{hwang2022tutel,
  title={Tutel: Adaptive Mixture-of-Experts at Scale},
  author={Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others},
  journal={arXiv preprint arXiv:2206.03382},
  year={2022}
}

@article{ben2019demystifying,
  title={Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
  author={Ben-Nun, Tal and Hoefler, Torsten},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={4},
  pages={1--43},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@article{xu2022survey,
  title={A survey on dynamic neural networks for natural language processing},
  author={Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2202.07101},
  year={2022}
}


@article{chen2022ta,
  title={TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training},
  author={Chen, Chang and Li, Min and Wu, Zhihua and Yu, Dianhai and Yang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22173--22186},
  year={2022}
}

@misc{nvidia_a100,
    title={Nvidia A100 GPU},
    url={https://www.nvidia.com/en-us/data-center/a100/}
}

@misc{enwik8,
    title={Enwik8 Dataset},
    url={https://huggingface.co/datasets/enwik8}
}


@inproceedings{li2023accelerating,
      author = {Jiamin Li and Yimin Jiang and Yibo Zhu and Cong Wang and Hong Xu},
      title = {Accelerating Distributed MoE Training and Inference with Lina},
      booktitle = {Proceedings of the USENIX Annual Technical Conference},
      year = {2023},
}

@article{Ouyangjpdc21,
  author       = {Shuo Ouyang and
                  Dezun Dong and
                  Yemao Xu and
                  Liquan Xiao},
  title        = {Communication optimization strategies for distributed deep neural
                  network training: {A} survey},
  journal      = {J. Parallel Distributed Comput.},
  volume       = {149},
  pages        = {52--65},
  year         = {2021},
}

@article{you2019fast,
  title={Fast deep neural network training on distributed systems and cloud TPUs},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={30},
  number={11},
  pages={2449--2462},
  year={2019},
  publisher={IEEE}
}

@inproceedings{stich2019local,
  title={Local SGD Converges Fast and Communicates Little},
  author={Stich, Sebastian Urban},
  booktitle={ICLR 2019-International Conference on Learning Representations},
  number={CONF},
  year={2019}
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@inproceedings{zhou2018adaptive,
  title={Adaptive quantization for deep neural network},
  author={Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{gdr,
    howpublished = {\url{https://docs.nvidia.com/cuda/gpudirect-rdma/index.html}},
    title        = {NVIDIA GPU Direct RDMA},
}

@inproceedings{fei2021efficient,
  title={Efficient sparse collective communication and its application to accelerate distributed deep learning},
  author={Fei, Jiawei and Ho, Chen-Yu and Sahu, Atal N and Canini, Marco and Sapio, Amedeo},
  booktitle={Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
  pages={676--691},
  year={2021}
}

@article{daily2018gossipgrad,
  title={Gossipgrad: Scalable deep learning using gossip communication based asynchronous gradient descent},
  author={Daily, Jeff and Vishnu, Abhinav and Siegel, Charles and Warfel, Thomas and Amatya, Vinay},
  journal={arXiv preprint arXiv:1803.05880},
  year={2018}
}

@inproceedings{peng2019generic,
  title={A generic communication scheduler for distributed dnn training acceleration},
  author={Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={16--29},
  year={2019}
}

@inproceedings{mai2015optimizing,
  title={Optimizing Network Performance in Distributed Machine Learning.},
  author={Mai, Luo and Hong, Chuntao and Costa, Paolo},
  booktitle={HotCloud},
  year={2015}
}

@inproceedings{chowdhury2014efficient,
  title={Efficient coflow scheduling with varys},
  author={Chowdhury, Mosharaf and Zhong, Yuan and Stoica, Ion},
  booktitle={Proceedings of the 2014 ACM conference on SIGCOMM},
  pages={443--454},
  year={2014}
}

@article{jayarajan2019priority,
  title={Priority-based parameter propagation for distributed DNN training},
  author={Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={132--145},
  year={2019}
}

@article{hashemi2019tictac,
  title={Tictac: Accelerating distributed deep learning with communication scheduling},
  author={Hashemi, Sayed Hadi and Abdu Jyothi, Sangeetha and Campbell, Roy},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={418--430},
  year={2019}
}

@inproceedings{mahajan2023better,
  title={Better Together: Jointly Optimizing $\{$ML$\}$ Collective Scheduling and Execution Planning using $\{$SYNDICATE$\}$},
  author={Mahajan, Kshiteej and Chu, Ching-Hsiang and Sridharan, Srinivas and Akella, Aditya},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={809--824},
  year={2023}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}


@inproceedings{ren2021zero,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training.},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}

@inproceedings{zheng2022alpa,
  title={Alpa: Automating Inter-and $\{$Intra-Operator$\}$ Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={559--578},
  year={2022}
}
